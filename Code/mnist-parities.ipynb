{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('./data/mnist.npz')\n",
    "if data_path.is_file():\n",
    "    print('Dataset found. Reading data...')\n",
    "    data_clump = np.load(data_path)\n",
    "    X_train_split, Y_train_split, X_test_split, Y_test_split = data_clump['arr_0'], data_clump['arr_1'], data_clump['arr_2'], data_clump['arr_3']\n",
    "else:\n",
    "    from keras.datasets import mnist\n",
    "    print('Dataset missing. Loading data...')\n",
    "    (X_train_split, Y_train_split), (X_test_split, Y_test_split) = mnist.load_data()\n",
    "    np.savez_compressed(data_path, X_train_split, Y_train_split, X_test_split, Y_test_split)\n",
    "X_train_split.shape, Y_train_split.shape, X_test_split.shape, Y_test_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def make_parity_data(\n",
    "    x: np.array, \n",
    "    y: np.array, \n",
    "    num_samples: int = NUM_SAMPLES,\n",
    "    num_cols: int = K\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    indices = np.random.choice(range(len(x)), (num_samples, num_cols), replace=True)\n",
    "    x_parity = torch.cat([\n",
    "        torch.cat([\n",
    "            torch.tensor(x[indices[i][j]].reshape(1, -1), dtype=torch.float32)/255 for j in range(num_cols)\n",
    "        ], dim=1) for i in range(num_samples)\n",
    "    ])\n",
    "    y_parity = torch.tensor(y[indices], dtype=torch.float32).sum(dim=1).reshape(-1, 1) % 2\n",
    "    return x_parity, y_parity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, Y_training = make_parity_data(X_train_split, Y_train_split)\n",
    "X_test, Y_test = make_parity_data(X_test_split, Y_test_split, num_samples=NUM_SAMPLES//5)\n",
    "X_training.shape, Y_training.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_cv, Y_training_cv = resample(X_training, Y_training, replace=False, n_samples=2000)\n",
    "X_training_cv.shape, Y_training_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_training, Y_training, test_size=0.2)\n",
    "X_train.shape, X_val.shape, Y_train.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, X_val_cv, Y_train_cv, Y_val_cv = train_test_split(X_training_cv, Y_training_cv, test_size=0.2)\n",
    "X_train_cv.shape, X_val_cv.shape, Y_train_cv.shape, Y_val_cv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.models import SimpleNN\n",
    "from scripts.metrics import BinaryAccuracy\n",
    "from scripts.train import train_model\n",
    "from scripts.utils import EarlyStopping, make_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [1, 2, 3, 4, 5]\n",
    "widths = [16, 32, 64]\n",
    "weight_decays = torch.logspace(-3, 3, 7)\n",
    "etas = [1e-4, 1e-3, 1e-2]\n",
    "batch_sizes = [16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0.0\n",
    "best_depth = None\n",
    "best_width = None\n",
    "best_weight_decay = None\n",
    "best_eta = None\n",
    "best_batch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(depths) * len(widths) * len(weight_decays) * len(etas) * len(batch_sizes)\n",
    "count = 0\n",
    "EPOCHS = 50\n",
    "\n",
    "print(f'Cross-validating across {total_count} models.\\n')\n",
    "\n",
    "for depth in depths:\n",
    "    for width in widths:\n",
    "        for weight_decay in weight_decays:\n",
    "            for eta in etas:\n",
    "                for batch_size in batch_sizes:\n",
    "                    count += 1\n",
    "                    model = SimpleNN(input_size=784*K, hidden_layers=depth, hidden_units=width).to(device)\n",
    "                    loss_fn = torch.nn.BCELoss()\n",
    "                    optimizer = torch.optim.Adam(params=model.parameters(), lr=eta, weight_decay=weight_decay)\n",
    "                    metric = BinaryAccuracy()\n",
    "                    train_dataloader = make_dataloader(X_train_cv, Y_train_cv, batch_size=batch_size, shuffle=True)\n",
    "                    val_dataloader = make_dataloader(X_val_cv, Y_val_cv, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                    history = train_model(\n",
    "                        model=model,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        val_dataloader=val_dataloader,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optimizer,\n",
    "                        metric=metric,\n",
    "                        epochs=50,\n",
    "                        verbose=0,\n",
    "                        device=device\n",
    "                    )\n",
    "                    curr_score = history['val_score'][-1]\n",
    "\n",
    "                    print(f'[{count}/{total_count}] depth={depth}, width={width}, lambda={weight_decay:.5f}, eta={eta}, batch size={batch_size} ===> validation score={curr_score:.6f}')\n",
    "                    if curr_score > best_score:\n",
    "                        best_score = curr_score\n",
    "                        best_depth = depth\n",
    "                        best_width = width\n",
    "                        best_weight_decay = weight_decay\n",
    "                        best_eta = eta\n",
    "                        best_batch_size = batch_size\n",
    "\n",
    "print(f'\\nValidation complete. Best validation score after {EPOCHS} epochs = {best_score:.6f}')\n",
    "print(f'Best configuration: depth={best_depth}, width={best_width}, lambda={best_weight_decay:.5f}, eta={best_eta}, batch size={best_batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_nn = SimpleNN(input_size=784*K, hidden_layers=best_depth, hidden_units=best_width).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=best_model_nn.parameters(), lr=best_eta, weight_decay=best_weight_decay)\n",
    "metric = BinaryAccuracy()\n",
    "early_stopper = EarlyStopping(patience=20, min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = make_dataloader(X_train, Y_train, batch_size=best_batch_size, shuffle=True)\n",
    "val_dataloader = make_dataloader(X_val, Y_val, batch_size=best_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(\n",
    "    model=best_model_nn,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    metric=metric,\n",
    "    epochs=500,\n",
    "    early_stopping=early_stopper,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import plot_train_history\n",
    "\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.test import predict\n",
    "\n",
    "preds_train, preds_val = predict(best_model_nn, X_train, device), predict(best_model_nn, X_val, device)\n",
    "score_train, score_val = metric(preds_train, Y_train), metric(preds_val, Y_val)\n",
    "score_train, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = predict(best_model_nn, X_test, device)\n",
    "score_test = metric(preds_test, Y_test)\n",
    "score_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from scripts.ntk import NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk = NTK(best_model_nn).get_ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_ntk = SVC(kernel=ntk)\n",
    "params_ntk = {\n",
    "    'C': np.logspace(-3, 3, 7)\n",
    "}\n",
    "\n",
    "gammas = np.logspace(-5, 5, 11, base=2).tolist()\n",
    "gammas.append('scale')\n",
    "gammas.append('auto')\n",
    "model_base_rbf = SVC()\n",
    "params_rbf = {\n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'gamma': gammas\n",
    "}\n",
    "\n",
    "scorer = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv_ntk = GridSearchCV(estimator=model_base_ntk, param_grid=params_ntk, scoring=scorer, n_jobs=5, refit=False, cv=5, verbose=3)\n",
    "model_cv_ntk.fit(X_train, Y_train.squeeze())\n",
    "best_params_ntk = model_cv_ntk.best_params_\n",
    "best_score_ntk = max(model_cv_ntk.cv_results_['mean_test_score'])\n",
    "best_params_ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv_rbf = GridSearchCV(estimator=model_base_rbf, param_grid=params_rbf, scoring=scorer, n_jobs=5, refit=False, cv=5, verbose=3)\n",
    "model_cv_rbf.fit(X_train, Y_train.squeeze())\n",
    "best_params_rbf = model_cv_rbf.best_params_\n",
    "best_score_rbf = max(model_cv_rbf.cv_results_['mean_test_score'])\n",
    "best_params_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_score_ntk > best_score_rbf:\n",
    "    best_model_km = SVC(C=best_params_ntk['C'], kernel=ntk, tol=1e-4)\n",
    "else:\n",
    "    best_model_km = SVC(C=best_params_rbf['C'], gamma=best_params_rbf['gamma'], tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_km.fit(X_train, Y_train.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train, preds_val = best_model_km.predict(X_train), best_model_km.predict(X_val)\n",
    "score_train, score_val = accuracy_score(Y_train, preds_train), accuracy_score(Y_val, preds_val)\n",
    "score_train, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = best_model_km.predict(X_test)\n",
    "score_test = accuracy_score(Y_test, preds_test)\n",
    "score_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
